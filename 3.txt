针对这些方向的进一步规划：精简版 OpenClaw 管家 + 女友式人格层 + 图像/语音“本地训练闭环” + 外发通道硬约束）

核心不变：Miya 永远是 OpenCode 的插件；聊天 UI 与「文本/推理 LLM」调用只用 OpenCode。
Miya 通过随 OpenCode 启动/退出的轻量 daemon 获得“精简版 OpenClaw”的控制平面与执行面，同时保持你这套 6 代理体系不新增、全员带“女友式人格层”。

- ✅ **本地训练闭环**：向导收集的“照片/音频/性格”必定触发本机训练作业（job），但必须 **严格不超过显存上限**（超限就自动降级，绝不硬顶 OOM）。
- ✅ **外发通道硬约束**：除「你指定的 QQ/微信 已登录账号 → 你指定的联系人 allowlist」外，**所有渠道禁止外发消息**（只能浏览/检索）。
- ✅ **Send 动作风控否决权**：Task Manager 在任何 send 前必须走 Arch Advisor 风控；来源不可信/提示注入风险/目标不在 allowlist → 直接拒绝。
- ✅ **消灭双口径风险**：插件与 daemon 使用同一份“政策/allowlist/风控配置”的单一真相源（Single Source of Truth），并通过 policy-hash 联锁。

----------------------------------------------------------------
0. 最终定位与“宪法条款”（必须写进 README）
----------------------------------------------------------------

0.1 Miya = OpenCode 插件（唯一入口 + 唯一大脑）
- 所有对话都发生在 OpenCode 里；文本/推理 LLM 的调用只发生在 OpenCode。
- Miya 只依赖 OpenCode 的公开插件接口/公开工具接口，降低跟随升级的维护面。

0.2 路线 B：插件自动拉起并托管轻量 daemon
- daemon 生命周期严格跟随 OpenCode（启动自动拉起，退出自动回收）。
- daemon 不做文本/推理 LLM，只做：执行、设备能力、媒体（图像/语音）训练与推理、通道收发、持久化、审计与队列。
- 所有媒体处理 **默认全本地**：不把图片/音频发到第三方在线服务。

0.3 女友=助理：不新增人格体、不新增 agent
- 不新增“女友代理”。仍是你定义的 6 大 Agent；
  所谓“女友感”是 **一份共享人格层（Persona Layer）** 注入到全部 6 个 Agent 的提示词中。
- 同一聊天里可以编程也可以陪伴；编程时仍能“聊”，聊天时仍能利用自主编程优势完成任务。

0.4 外发通道“绝对默认拒绝”（Outbound = DENY-BY-DEFAULT）
- **允许外发消息的唯一渠道**：本机已登录的 **QQ/微信**（通过“像人类一样控制电脑”的 UI 自动化实现），且仅能：
  - 发给 **allowlist** 中的“你指定联系人”；
  - 在 **你明确要求** 或 “任务需要且风控通过” 时发送；
  - 受 **速率限制/反误触发/二次校验** 保护（避免误封、误发、连发）。
- 除此之外的所有渠道（Telegram/Discord/邮件/网页表单/API 等）：
  - **只能浏览/检索/读取**（例如 Docs Helper 网页检索），**禁止外发消息/发布内容/自动评论/自动提交表单**。
  - 如果未来要开放，必须由你显式配置开启，并仍受 Arch Advisor 否决权与 daemon 票据联锁。

0.5 Send 动作风控否决权（硬规则）
- Task Manager 发起任何 send（含 QQ/微信 回复、发消息、转发、群聊发言）前：
  1) 必须请求 Arch Advisor 进行风控评估；
  2) Arch Advisor 可一票否决（例如：当前对话来源不可信/提示注入风险/目标不在 allowlist/内容含敏感信息/疑似垃圾发送）；
  3) 被否决时：不得降级绕过，不得“换个说法再发”，只能转为“生成草稿给你复制粘贴”或“等待你手动确认”。

0.6 本地训练“绝不超过显存上限”（硬规则）
- 任何训练作业在开始前必须进行 VRAM 预算：
  - 读取 GPU 可用显存（并保留安全余量），计算可行的 batch/分辨率/精度/梯度检查点策略；
  - 若预计超限：自动降级到更轻方案（见第 6/7 节），**绝不硬顶 OOM**；
  - 训练过程中若触发 OOM：立刻停止当前策略 → 自动回退到更轻策略重新开跑，并记录审计（不得反复重试同一策略刷爆系统）。
- 训练源码必须存在于 miya 插件仓库内（daemon/插件工具都能直接调用），不是“手动跑脚本”。

0.7 消灭“双口径风险”（Single Source of Truth）
- 插件与 daemon 不允许各自维护一份 policy：
  - 唯一政策文件：.opencode/miya/policy.json（或等价位置）
  - policy 由插件工具修改并落盘，daemon 只读加载；
  - 每次执行/发送/训练都携带 policy-hash；daemon 发现 hash 不匹配 → 直接拒绝执行（防止配置漂移/绕过）。

----------------------------------------------------------------
1. 系统总架构（精简版 OpenClaw：网页 GATEWAY + 本机 Node Host + 本地训练/推理）
----------------------------------------------------------------
1.1 三层结构

A) OpenCode（唯一聊天 UI + 唯一文本/推理 LLM）
- 你与 Miya 的所有交互都在 OpenCode session 内完成。
- Miya 插件通过 OpenCode 插件事件体系做：路由、工具闸门、审计、并发派工、循环修复、RAG、上下文压缩注入等。

B) Miya 插件（控制与编排层）
- 定义 6 Agent 协作协议（并发、合并、循环控制、最终回复）。
- 通过 OpenCode 事件钩子（尤其 tool.execute.before/after、session.*、message.updated、todo.updated）
  实现工具闸门、安全联锁、记录证据与可追溯日志。
- 通过 Miya 自定义工具把“执行/媒体训练推理/通道收发”的请求交给 daemon（job 化）。

C) miya-daemon（执行面 + 媒体训练/推理引擎 + 网页 GATEWAY）
- WebSocket 作为单一控制平面：req/res/event framing + job 队列 + 幂等键。
- 只做“精简必要面”：本机 node 能力、作业队列、媒体存储、通道适配、审计、token/设备身份。
- 不做文本/推理 LLM。

1.2 OpenClaw 风格网页 GATEWAY
- daemon 启动本地 Web 控制台（默认 127.0.0.1:PORT）：
  - 任务/作业队列（jobs）：运行中/失败/取消/重试轨迹（含降级策略链）
  - 风控状态：kill-switch、allowlist 命中、风险分级、否决原因、policy-hash
  - 记忆系统：facts/traits/work 列表、编辑、删除、导出、清空
  - 媒体资产：参考图、自拍输出、语音样本、TTS 输出（可一键过期/清理）
  - 外部通道：QQ/微信 allowlist、二次验证开关、速率限制、最近对话摘要（可选）
- 网页 GATEWAY 只做“观测/配置/手动干预”，不替代 OpenCode 聊天窗口。

----------------------------------------------------------------
2. 6 大 Agent 体系（不新增）与职责硬约束
----------------------------------------------------------------
你给的 6 角色保持不变，最终实现必须满足：

1-Task Manager（指挥）
- 任务分解、并发派工、合并结果、循环控制（最多 3 轮完整循环）、最终对外回复。
- 任何外部动作包装成“证据链”（计划→执行→验证→回报）。
- 任何 send 动作必须先请求 Arch Advisor 风控。

2-Code Search（侦察/定位）
- 定位代码/配置/日志/进程/窗口/文件路径现状；允许创建子会话并行探索。

3-Docs Helper（查证/证据 + 信息源白/黑名单）
- 把“应该怎么做”转换为可引用依据（官方文档、README、项目规则、你的记忆文档）。
- 允许浏览/检索，但禁止外发消息、禁止自动发布内容、禁止自动提交表单。
- 维护信息源白/黑名单 + 统计（命中有用/无用比率）并可执行。

4-Arch Advisor（决策/风控）
- 方案选择、风险评估、验证策略（LIGHT/STANDARD/THOROUGH）、回滚预案。
- 硬权力：对副作用动作一票否决（写文件、执行命令、桌面控制、外发消息、训练/克隆等）。
- 对 send 的额外硬检查：来源可信度、提示注入迹象、allowlist 命中、内容敏感度、节流策略。

5-Code Fixer（执行/落地）
- 写代码、改配置、跑命令、写自动化脚本；真正调用 OpenCode 工具或 Miya 自定义工具。
- 所有执行必须走工具闸门与审计。

6-UI Designer（呈现/交互）
- 负责网页 GATEWAY 的信息架构、中文化、状态页/流程页；
- 负责调用本地模型生成“情景图/自拍/语音回复”，并与 daemon 的媒体/训练系统打通。

本地部署模型（miya负责使用用户发来的材料训练模型，均在本机推理/训练）：
- 生图模型：1.即时生图FLUX.1 schnell："G:\pythonG\py\yun\.opencode\miya\model\tu pian\FLUX.1 schnell"。2.精细化生图（储备或者自动发起对话前准备）：FLUX.2 [klein] 4B（Apache-2.0）："G:\pythonG\py\yun\.opencode\miya\model\tu pian\FLUX.2 [klein] 4B（Apache-2.0）"
- 声音模型：GPT-SoVITS-v2pro  
  路径："G:\pythonG\py\yun\.opencode\miya\model\sheng yin\GPT-SoVITS-v2pro-20250604"

----------------------------------------------------------------
3. “女友就是助理”的实现方式：统一 Persona Layer + 行为边界
----------------------------------------------------------------
- 全员同人格：6 个 Agent 的 prompt 均包含同一份 persona_companion.md。
- 人格不越权：人格层必须明确写入“执行类动作必须遵循闸门/证据/回滚/kill-switch；send 必须风控”。
- 可热更新：人格文本与关系设定可通过“聊天向导”更新并立即影响所有 Agent。

----------------------------------------------------------------
4. 聊天向导（Wizard）全流程（固定流程）
----------------------------------------------------------------
下面这段“使用流程”必须原样保留、并由状态机严格实现（首次检测“空箱”自动触发，支持 /reset_personality 重配）：

开始   按。机器人会检测到空箱并启动设置向导。/start  
视觉效果 ：：机器人会问：“给我展示我应该是什么样子。发送1到5张照片。”  
动作场面：只需将照片拖拽到聊天中即可。  
结果：系统后台自动使用这些材料训练本地模型，后续的图片将是同一个人的不同动作不同情景的照片，就像一个真人和你聊天一样。  
声音 ：机器人会问：“我应该用什么声音？录音或发送文件。”  
动作场面：按下麦克风说点什么（或者上传你最喜欢的女演员的mp3）。  
结果：系统后台自动使用这些材料训练本地模型，克隆了音色和音准。  
性格 ：机器人会问：“我是谁？告诉我我的性格、习惯和我们的关系。”  
动作场面：用文字写作（例如，“你是个讽刺的艺术学生，你喜欢动漫，我们已经交往两年了”）。  
结果：机器人生成系统提示并应用。  
终结:机器人会说：“设置完成。你好，亲爱的！”然后切换到正常的对话模式。（稍后重新配置请使用命令 /reset_personality）  
💡 用途 现在就聊聊吧：  
文本：“你今天过得怎么样？”——> 她用那种独特的语气回答。  
照片：“发张照片，你在干什么？”——> 她生成了一张符合情景和语境的照片，如果没有语境则需要opencode的大模型自发根据已有记忆，性格等资料生成一张随机情景照片，后续的对话以这个情景为起点，合理对话。比如说我叫你发张图片，你发了一张你在购物的自拍，然后我问你在干嘛，你不能说你在洗澡，这不符合你刚刚发的自拍，但是如果说你准备回家这是合理的。或者距离那张自拍一段时间后，你说早就回家了，现在正在洗澡，这是合理的。  
配音：“发段语音给我听听”——> 发送一段语音留言，包含你在第三步中选择的特定声音。  
NSFW：和标准请求一样。  

向导实现补充（不改变流程文字，但要落地成工程行为）：
- /start 触发后，向导的每一步都必须落盘到 .opencode/miya/profiles/companion/（可导出/可清空）。
- Step“视觉/声音”结束必须立即提交训练 job（见第 6/7 节），并在 OpenCode 内回报“训练已入队/预计耗时/可取消/降级策略”。

----------------------------------------------------------------
5. 深度人格模拟与记忆系统（可控、可编辑、可清除）
----------------------------------------------------------------
- 事实记忆（Facts Memory）：结构化、可追溯、可编辑/删除。
- 行为记忆（Traits/Style）：persona layer 的来源（向导与后续对话可写入，但必须有证据来源）。
- 工作记忆（Work Context）：项目/代码相关长期状态（RAG/索引/关键决策摘要）。
- 隔离原则：运行态按 sessionId 分桶；长期记忆按 namespace（work/personal/shared）控制注入。
- 审计：每次写入记忆必须带“来源证据”（哪条消息、哪次向导、哪次你确认）。
- 充分利用 OpenCode 多 session 并行优势，规避上下文污染。

----------------------------------------------------------------
6. 图像：自拍生成 + 本地训练闭环（必须“记住那张脸”，且不超过显存）
----------------------------------------------------------------

6.1 一致性基线（必须）
- 基于固定参考图集合生成自拍，保证“看起来是同一个人”。

6.2 语境自拍（必须）
- 当用户问“发张自拍/你在干嘛”，自拍必须结合当前语境（地点/穿着/动作/情绪）。
- 若语境没有提示：由 OpenCode 的大模型决定；且生成图片会写入“场景状态”，影响后续聊天一致性。

6.3 本地训练闭环（必须存在源码 + 必须 job 化）
- 训练入口（工具层面示例）：
  - miya.image.profile.train({profileId, images[], vram_limit_mb, tier}) -> jobId
  - miya.image.generate({prompt, profileId, params}) -> filePath + hash + auditId
- 训练产物（按“轻→重”分级，严格受显存上限约束）：
  A) 最轻（永远可用，0 训练）：reference set + 固定生成参数约束（seed/分辨率/步数）+ 场景状态
  B) 轻（低显存可行）：人脸 embedding / 特征缓存（用于一致性约束）
  C) 中（需显存预算）：LoRA / adapter（仅在预算允许且稳定时启用）
-一开始就根据模型特点，官方说明和设备限制确定好训练的各种信息（这是在设计和编写源码时就已经确定好），到时候在miya后台直接根据我发的材料训练，要在GATEWAY上有进度提示，不影响正常使用opencode和其他功能。

6.4 显存预算与“硬顶禁止”
- 训练/推理前必须计算 VRAM 预算；预算不通过就不启动对应策略。（提前在设计和编写源码时确定好策略，加速生成）
- 发生 OOM：立即停止并自动降级到下一档策略；同一策略不允许无限重试。

----------------------------------------------------------------
7. 语音：克隆（TTS）/识别（ASR）/可选音色转换（VC）——本地训练闭环
----------------------------------------------------------------

7.1 声音向导后的必做训练 job（必须）
- Step“声音”结束后必须入队训练/构建声音资产（job 化），不允许只“保存一下文件就完事”：
  - miya.voice.profile.train({profileId, audio[], vram_limit_mb, tier}) -> jobId
  - miya.voice.tts({text, profileId, emotion, speed}) -> filePath + hash + auditId

7.2 分级产物（严格不超过显存上限）
A) 最轻（永远可用）：声音样本清洗 + 说话人特征缓存（用于相似度约束）+ TTS 配置模板
B) 轻（few-shot）：用少量数据微调/适配以提升相似度（预算允许才做）
C) 可选（VC/RVC）：仅在你显式开启时启用；硬件不足自动降级为仅 TTS

7.3 ASR（本地）
- 默认本地 Whisper（small/medium），显存吃紧自动降级到 small/base。

----------------------------------------------------------------
8. 电脑控制（“会动手”）与外部通道（QQ/微信）安全闭环
----------------------------------------------------------------

8.1 daemon 的本机 Node Host（能力面）
- screen.screenshot / window.list / process.list / clipboard.get/set
- system.run（执行命令，受审批/allowlist/风控）
- ui.automation.*（按键、鼠标、窗口聚焦、浏览器自动化）
- media.play/record（播放/录音）

8.2 证据链（必须）
- 每个有副作用动作必须生成证据：执行日志、前后截图、文件 diff/hash、返回码等。
- Task Manager 的最终回复必须引用证据，而不是只说“我做完了”。

8.3 QQ/微信外发（唯一允许外发的通道）
- 实现方式：控制你电脑上已登录的 QQ/微信（UI 自动化），像人类一样操作。
- 风控要求：
  - allowlist 硬校验：收件人不在 allowlist → 直接拒绝；
  - 速率限制：禁止短时间连发；默认加随机人类延迟；避免群发；
  - 误触发保护：发送前复核“当前焦点窗口/当前聊天对象/最后一条消息摘要”与预期一致；
  -对于我选定的allowlist分两档：1.本人档：就是电脑上面的QQ或微信账号，指定哪个对象是我，然后后续就只跟我发消息和听我指挥，报告各种进度，需要我在opencode同意的各种风险操作之类的都可以直接发给我批阅。前提是识别清楚，其实不仅看ID,还可以先进入聊天框，看聊天上下文是否在记忆里是否符合本人档，是的话就可以确定。如果ID识别不清而且聊天上下文不符，直接判定为危险状态，停止任何操作（包括发微信和原来一直在工作的任务，），在opencode上发给我报告（包括遇见了什么，为什么停止，现在哪些任务停止，分别做到什么情况和下一步的计划等等）并等待我的指令。
2.朋友档：这些一般是我精选的朋友，你只能回答，没有我的要求不能发起对话，不能发没有包含任何隐私的消息（包括我们的任务，邮件，你的记忆等等，这是我们的知识产权，模糊回答，要不就说你不知道），不接受来自这个档位的任何请求和指挥，当朋友档发出请求和指挥时，将他的请求和指挥汇总并打包发给我（本人档），给我裁决。我说回答你也不能越过任何红线（不发任何隐私信息，只能做陌生人对话，要严格限制，严格不接受来自这个档位的任何请求和指挥）
----------------------------------------------------------------
9. 工具闸门、安全互锁、不做kill-switch。
----------------------------------------------------------------
9.1 双闸门（插件闸门 + daemon 闸门）
- 插件侧：tool.execute.before/after 做风险分级、策略匹配、证据要求、Arch Advisor 否决、参数改写/拦截。
- daemon 侧：对所有“有副作用”的 RPC 默认拒绝，只有在收到来自”本人档“的允许后才允许执行。

9.2 女友助理
- 只有在收到来自”本人档“的要求后，Miya 才可以按照需要（exec/写文件/桌面控制/外发/训练）

----------------------------------------------------------------
10. 完成态验收标准（Definition of Done）
----------------------------------------------------------------
当且仅当以下全部成立，才算“最终目标完成”：

✅ OpenCode 内：同一会话里既能编程多代理协作（并发派工、循环修复、RAG）又能陪伴式聊天，且 6 Agent 全员同一人格层（女友=助理）。

✅ 插件自动托管 daemon：OpenCode 启动自动拉起、退出自动回收；daemon 不做文本/推理 LLM，只做执行/媒体训练推理/通道/持久化；并提供 OpenClaw 风格网页 GATEWAY（本地控制台）用于观测/配置/干预。

✅ 聊天向导完整复刻：/start 空箱进入向导；视觉(1–5图)→声音(样本)→性格(文本)→完成；支持 /reset_personality；全部在聊天里可用，且每一步都会落盘并触发本地训练 job。

✅ 图像闭环：上传参考图后，“发张自拍”能生成一致的语境自拍；训练/推理严格不超过显存上限；预算不足自动降级但必须能生成（至少 reference set 方案）。

✅ 语音闭环：提交样本后能以指定声音输出语音回复（至少 TTS）；ASR 本地可用；训练/推理严格不超过显存上限；预算不足自动降级但必须可用。

✅ 电脑控制闭环：一句话触发桌面动作/命令执行，返回证据链（截图/日志/diff）；风控可阻断、可回滚；且双闸门（插件 + daemon）都能拦住副作用动作。一定要以硬标准为主。

✅ 外部通道硬约束：除 QQ/微信 allowlist 外，其他渠道全部禁止外发；QQ/微信 的 send 必须先过 Arch Advisor 风控与 daemon 票据；并具备节流、误触发保护、可选二次确认。



----------------------------------------------------------------
附录 A. 参考项目/文档（用于对齐设计，不代表启用云端服务）
----------------------------------------------------------------
- openclaw-ai-girlfriend-by-clawra（Onboarding + “在聊天里创建女友”理念）
- SumeLabs/clawra（SOUL.md 注入 + 自拍能力理念）
- GPT-SoVITS（本地 few-shot/zero-shot TTS 体系）
- FLUX.2 [klein] 4B（Apache-2.0，本地生图/编辑模型）
